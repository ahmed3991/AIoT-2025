{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d102adb5",
   "metadata": {},
   "source": [
    "\n",
    "# AI for IoT — TP2: Pipelines, Resource Profiling, and ESP32 Deployment Readiness\n",
    "\n",
    "This notebook builds on TP1 to:\n",
    "1) Wrap preprocessing + models into **sklearn Pipelines**  \n",
    "2) Measure **model size** and **inference time**  \n",
    "3) Draft a short **deployment analysis** for **ESP32** (≈520 KB SRAM, MBs of Flash)\n",
    "\n",
    "> Run cells in order. If you use Kaggle download, upload your `kaggle.json` first in Colab (left sidebar → Files → Upload).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7411bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================\n",
    "# 0) Setup & Installs\n",
    "# =====================\n",
    "!pip -q install xgboost seaborn\n",
    "\n",
    "import os, sys, io, time, pickle, warnings, zipfile\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "print(\"Environment ready ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25d3614",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Data Acquisition\n",
    "Preferred: download from Kaggle (`deepcontractor/smoke-detection-dataset`).  \n",
    "Alternative: manually upload a CSV and set `csv_path` accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f691a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Try to configure Kaggle if kaggle.json is present\n",
    "use_kaggle = False\n",
    "kaggle_json = Path('/content/kaggle.json')  # Colab default suggestion\n",
    "dataset_dir = Path('/content/data')\n",
    "dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "csv_path = dataset_dir / 'smoke_detection.csv'\n",
    "\n",
    "if kaggle_json.exists():\n",
    "    use_kaggle = True\n",
    "    !mkdir -p ~/.kaggle\n",
    "    !cp /content/kaggle.json ~/.kaggle/\n",
    "    !chmod 600 ~/.kaggle/kaggle.json\n",
    "    print(\"Kaggle API configured ✅\")\n",
    "else:\n",
    "    print(\"⚠️ kaggle.json not found. You can still upload a CSV manually.\")\n",
    "\n",
    "if use_kaggle:\n",
    "    # Download and extract\n",
    "    !kaggle datasets download -d deepcontractor/smoke-detection-dataset -p /content/data -q\n",
    "    for zf in dataset_dir.glob(\"*.zip\"):\n",
    "        with zipfile.ZipFile(zf, 'r') as z:\n",
    "            z.extractall(dataset_dir)\n",
    "    # Pick a CSV\n",
    "    candidates = list(dataset_dir.glob(\"*.csv\"))\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(\"No CSV found after unzip. Please check dataset.\")\n",
    "    pref = [p for p in candidates if 'smoke' in p.name.lower()]\n",
    "    target = pref[0] if pref else candidates[0]\n",
    "    target.rename(csv_path)\n",
    "    print(f\"Data ready at: {csv_path}\")\n",
    "else:\n",
    "    # If you manually uploaded a CSV, set its path here:\n",
    "    # Example: csv_path = Path('/content/your_uploaded_file.csv')\n",
    "    if csv_path.exists():\n",
    "        print(f\"Using existing local CSV: {csv_path}\")\n",
    "    else:\n",
    "        print(\"Upload a CSV to /content and set csv_path to its path if needed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25374f2c",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Load & Quick Clean\n",
    "- Fill numeric NaNs with median\n",
    "- Auto-detect target column among common names (edit if needed)\n",
    "- Drop likely time columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7c9219",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head())\n",
    "\n",
    "# Fill numeric NaNs\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
    "\n",
    "# Detect target\n",
    "possible_target_names = [\n",
    "    \"Fire Alarm\", \"fire_alarm\", \"FIRE_ALARM\", \"Target\", \"target\",\n",
    "    \"label\", \"Label\", \"fireAlarm\", \"smoke_detected\", \"Smoke_Detected\"\n",
    "]\n",
    "target_col = None\n",
    "for name in possible_target_names:\n",
    "    if name in df.columns:\n",
    "        target_col = name\n",
    "        break\n",
    "\n",
    "if target_col is None:\n",
    "    raise ValueError(\"Target column not found automatically. Please set `target_col` manually.\")\n",
    "\n",
    "print(\"✅ Target column:\", target_col)\n",
    "\n",
    "# Drop likely time columns\n",
    "drop_like = [\"UTC\", \"Time\", \"Date\"]\n",
    "to_drop = [c for c in df.columns if any(k.lower() in c.lower() for k in drop_like) and c != target_col]\n",
    "df = df.drop(columns=to_drop, errors=\"ignore\")\n",
    "print(\"Dropped (if any):\", to_drop)\n",
    "\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(\"Train/Test:\", X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3880b9f",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Pipelines\n",
    "We standardize features and train two models:\n",
    "- **LR-Pipeline**: `StandardScaler` → `LogisticRegression`\n",
    "- **XGB-Pipeline**: `StandardScaler` → `XGBClassifier` (kept for consistency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f95de00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Logistic Regression pipeline\n",
    "lr_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(max_iter=1000, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# XGBoost pipeline\n",
    "xgb_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric=\"logloss\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Fit\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "def evaluate(model, name):\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f\"=== {name} ===\")\n",
    "    print(f\"Accuracy: {acc:.4f} | F1: {f1:.4f}\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d')\n",
    "    plt.title(f\"Confusion Matrix - {name}\")\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "    return acc, f1\n",
    "\n",
    "acc_lr, f1_lr = evaluate(lr_pipeline, \"LR-Pipeline\")\n",
    "acc_xgb, f1_xgb = evaluate(xgb_pipeline, \"XGB-Pipeline\")\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Model\": [\"LR-Pipeline\", \"XGB-Pipeline\"],\n",
    "    \"Accuracy\": [acc_lr, acc_xgb],\n",
    "    \"F1\": [f1_lr, f1_xgb]\n",
    "})\n",
    "display(metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f1e947",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Resource Profiling — Model Size (KB)\n",
    "We serialize each pipeline with `pickle` and measure file size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd4131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "with open(\"lr_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(lr_pipeline, f)\n",
    "with open(\"xgb_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(xgb_pipeline, f)\n",
    "\n",
    "size_lr_kb = os.path.getsize(\"lr_model.pkl\") / 1024\n",
    "size_xgb_kb = os.path.getsize(\"xgb_model.pkl\") / 1024\n",
    "\n",
    "size_df = pd.DataFrame({\n",
    "    \"Model\": [\"LR-Pipeline\", \"XGB-Pipeline\"],\n",
    "    \"Size (KB)\": [size_lr_kb, size_xgb_kb]\n",
    "})\n",
    "display(size_df)\n",
    "print(f\"Files saved in working dir: lr_model.pkl ({size_lr_kb:.1f} KB), xgb_model.pkl ({size_xgb_kb:.1f} KB)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333031e6",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Resource Profiling — Inference Time\n",
    "Measure total prediction time on the full test set and average per-sample time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdbd482",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def measure_inference(model, X, repeats=3):\n",
    "    # Use repeats to smooth fluctuations\n",
    "    totals = []\n",
    "    for _ in range(repeats):\n",
    "        start = time.time()\n",
    "        _ = model.predict(X)\n",
    "        totals.append(time.time() - start)\n",
    "    total = np.mean(totals)\n",
    "    per_sample_ms = (total / len(X)) * 1000.0\n",
    "    return total, per_sample_ms\n",
    "\n",
    "tot_lr, ps_lr = measure_inference(lr_pipeline, X_test)\n",
    "tot_xgb, ps_xgb = measure_inference(xgb_pipeline, X_test)\n",
    "\n",
    "time_df = pd.DataFrame({\n",
    "    \"Model\": [\"LR-Pipeline\", \"XGB-Pipeline\"],\n",
    "    \"Total Test Inference Time (s)\": [tot_lr, tot_xgb],\n",
    "    \"Single Inference Time (ms)\": [ps_lr, ps_xgb]\n",
    "})\n",
    "display(time_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f79fa86",
   "metadata": {},
   "source": [
    "\n",
    "## 6) ESP32 Deployment Readiness — Auto Narrative\n",
    "This section drafts a short analysis referencing your measured sizes and timings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7730cd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ESP32_SRAM_KB = 520  # typical ballpark for total SRAM\n",
    "REALTIME_PERIOD_S = 1.0  # e.g., need to infer every 1 second\n",
    "\n",
    "choice = \"LR-Pipeline\" if (ps_lr <= REALTIME_PERIOD_S*1000 and size_lr_kb <= ESP32_SRAM_KB) else \"XGB-Pipeline\"\n",
    "# Simplistic rule; you may refine based on your needs (Flash vs SRAM distinction, etc.)\n",
    "\n",
    "analysis = f'''\n",
    "**Memory Feasibility:**  \n",
    "- LR size ≈ {size_lr_kb:.1f} KB | XGB size ≈ {size_xgb_kb:.1f} KB.  \n",
    "Typical ESP32 offers ~{ESP32_SRAM_KB} KB SRAM (runtime) and several MB Flash (storage).  \n",
    "Smaller models are easier to store and load; LR is usually lighter than XGB.\n",
    "\n",
    "**Time Efficiency:**  \n",
    "- LR avg single inference ≈ {ps_lr:.3f} ms  \n",
    "- XGB avg single inference ≈ {ps_xgb:.3f} ms  \n",
    "If the application needs 1 prediction every {REALTIME_PERIOD_S:.0f} s, both should be << 1000 ms to be safe.\n",
    "\n",
    "**Conclusion:**  \n",
    "Based purely on efficiency and embedded constraints, **{choice}** is the safer default for ESP32.  \n",
    "If XGB is preferred for accuracy, consider pruning, lowering n_estimators/max_depth, or quantization. Also reduce features where possible.\n",
    "'''\n",
    "print(analysis)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
