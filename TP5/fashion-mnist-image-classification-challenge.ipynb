{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e9b4e1b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-04T11:54:02.378436Z",
     "iopub.status.busy": "2025-11-04T11:54:02.377752Z",
     "iopub.status.idle": "2025-11-04T11:54:04.120455Z",
     "shell.execute_reply": "2025-11-04T11:54:04.119641Z"
    },
    "papermill": {
     "duration": 1.751713,
     "end_time": "2025-11-04T11:54:04.121855",
     "exception": false,
     "start_time": "2025-11-04T11:54:02.370142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/fashionmnist/t10k-labels-idx1-ubyte\n",
      "/kaggle/input/fashionmnist/t10k-images-idx3-ubyte\n",
      "/kaggle/input/fashionmnist/fashion-mnist_test.csv\n",
      "/kaggle/input/fashionmnist/fashion-mnist_train.csv\n",
      "/kaggle/input/fashionmnist/train-labels-idx1-ubyte\n",
      "/kaggle/input/fashionmnist/train-images-idx3-ubyte\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f114a2b",
   "metadata": {
    "papermill": {
     "duration": 0.004486,
     "end_time": "2025-11-04T11:54:04.131376",
     "exception": false,
     "start_time": "2025-11-04T11:54:04.126890",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ğŸ“ Task 1.1: Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85c5856c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T11:54:04.142013Z",
     "iopub.status.busy": "2025-11-04T11:54:04.141606Z",
     "iopub.status.idle": "2025-11-04T11:54:22.029411Z",
     "shell.execute_reply": "2025-11-04T11:54:22.028325Z"
    },
    "papermill": {
     "duration": 17.894851,
     "end_time": "2025-11-04T11:54:22.030790",
     "exception": false,
     "start_time": "2025-11-04T11:54:04.135939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 11:54:06.072204: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762257246.327687      13 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762257246.395242      13 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "\u001b[1m29515/29515\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "\u001b[1m26421880/26421880\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "\u001b[1m5148/5148\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "\u001b[1m4422102/4422102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "MLP train shape: (60000, 784)\n",
      "CNN train shape: (60000, 28, 28, 1)\n",
      "MLP test shape: (10000, 784)\n",
      "CNN test shape: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# 1ï¸âƒ£ Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# 2ï¸âƒ£ Load the Fashion-MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# 3ï¸âƒ£ Normalize the pixel values to [0, 1]\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# 4ï¸âƒ£ Prepare data for MLP\n",
    "train_images_mlp = train_images.reshape(-1, 28*28)  # Flatten 28x28 images to 784-element vectors\n",
    "test_images_mlp = test_images.reshape(-1, 28*28)\n",
    "\n",
    "# 5ï¸âƒ£ Prepare data for CNN\n",
    "train_images_cnn = train_images.reshape(-1, 28, 28, 1)  # Add channel dimension\n",
    "test_images_cnn = test_images.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# 6ï¸âƒ£ Print shapes to verify\n",
    "print(\"MLP train shape:\", train_images_mlp.shape)\n",
    "print(\"CNN train shape:\", train_images_cnn.shape)\n",
    "print(\"MLP test shape:\", test_images_mlp.shape)\n",
    "print(\"CNN test shape:\", test_images_cnn.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d05b917",
   "metadata": {
    "papermill": {
     "duration": 0.006485,
     "end_time": "2025-11-04T11:54:22.042992",
     "exception": false,
     "start_time": "2025-11-04T11:54:22.036507",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ğŸ“ Task 2.1: Implement and Compile the MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b89f3b3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T11:54:22.059550Z",
     "iopub.status.busy": "2025-11-04T11:54:22.058996Z",
     "iopub.status.idle": "2025-11-04T11:54:22.164930Z",
     "shell.execute_reply": "2025-11-04T11:54:22.164308Z"
    },
    "papermill": {
     "duration": 0.113857,
     "end_time": "2025-11-04T11:54:22.166078",
     "exception": false,
     "start_time": "2025-11-04T11:54:22.052221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "2025-11-04 11:54:22.072716: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">200,960</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ flatten (\u001b[38;5;33mFlatten\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚       \u001b[38;5;34m200,960\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚        \u001b[38;5;34m32,896\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             â”‚         \u001b[38;5;34m1,290\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">235,146</span> (918.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m235,146\u001b[0m (918.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">235,146</span> (918.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m235,146\u001b[0m (918.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "\n",
    "# 1ï¸âƒ£ Define the MLP model\n",
    "mlp_model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),          # Input layer: flatten 28x28 image\n",
    "    Dense(256, activation='relu'),          # First hidden layer\n",
    "    Dense(128, activation='relu'),          # Second hidden layer\n",
    "    Dense(10, activation='softmax')         # Output layer for 10 classes\n",
    "])\n",
    "\n",
    "# 2ï¸âƒ£ Compile the model\n",
    "mlp_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 3ï¸âƒ£ Print model summary\n",
    "mlp_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa54033",
   "metadata": {
    "papermill": {
     "duration": 0.005733,
     "end_time": "2025-11-04T11:54:22.177736",
     "exception": false,
     "start_time": "2025-11-04T11:54:22.172003",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ğŸ“ Task 2.2: Implement and Compile the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26fff1aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T11:54:22.190616Z",
     "iopub.status.busy": "2025-11-04T11:54:22.190318Z",
     "iopub.status.idle": "2025-11-04T11:54:22.260679Z",
     "shell.execute_reply": "2025-11-04T11:54:22.259995Z"
    },
    "papermill": {
     "duration": 0.078519,
     "end_time": "2025-11-04T11:54:22.262038",
     "exception": false,
     "start_time": "2025-11-04T11:54:22.183519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">800</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">51,264</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m16\u001b[0m)     â”‚           \u001b[38;5;34m160\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m16\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚         \u001b[38;5;34m4,640\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m32\u001b[0m)       â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m800\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚        \u001b[38;5;34m51,264\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             â”‚           \u001b[38;5;34m650\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">56,714</span> (221.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m56,714\u001b[0m (221.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">56,714</span> (221.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m56,714\u001b[0m (221.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# 1ï¸âƒ£ Define the CNN model\n",
    "cnn_model = Sequential([\n",
    "    # Convolutional Block 1\n",
    "    Conv2D(16, kernel_size=3, activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    \n",
    "    # Convolutional Block 2\n",
    "    Conv2D(32, kernel_size=3, activation='relu'),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    \n",
    "    # Classifier\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# 2ï¸âƒ£ Compile the model\n",
    "cnn_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 3ï¸âƒ£ Print model summary\n",
    "cnn_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aafcd6f",
   "metadata": {
    "papermill": {
     "duration": 0.006314,
     "end_time": "2025-11-04T11:54:22.275188",
     "exception": false,
     "start_time": "2025-11-04T11:54:22.268874",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ğŸ“ Task 3.1: Train the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d0de07e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T11:54:22.289568Z",
     "iopub.status.busy": "2025-11-04T11:54:22.289015Z",
     "iopub.status.idle": "2025-11-04T11:55:22.028415Z",
     "shell.execute_reply": "2025-11-04T11:55:22.027412Z"
    },
    "papermill": {
     "duration": 59.748631,
     "end_time": "2025-11-04T11:55:22.030270",
     "exception": false,
     "start_time": "2025-11-04T11:54:22.281639",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m844/844\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 14ms/step - accuracy: 0.7062 - loss: 0.8288 - val_accuracy: 0.8423 - val_loss: 0.4274\n",
      "Epoch 2/5\n",
      "\u001b[1m844/844\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.8550 - loss: 0.4012 - val_accuracy: 0.8688 - val_loss: 0.3558\n",
      "Epoch 3/5\n",
      "\u001b[1m844/844\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.8768 - loss: 0.3388 - val_accuracy: 0.8848 - val_loss: 0.3208\n",
      "Epoch 4/5\n",
      "\u001b[1m844/844\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.8919 - loss: 0.3026 - val_accuracy: 0.8835 - val_loss: 0.3169\n",
      "Epoch 5/5\n",
      "\u001b[1m844/844\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.8961 - loss: 0.2838 - val_accuracy: 0.8810 - val_loss: 0.3262\n"
     ]
    }
   ],
   "source": [
    "# 1ï¸âƒ£ Train the CNN model\n",
    "cnn_history = cnn_model.fit(\n",
    "    train_images_cnn,       # Images with channel dimension\n",
    "    train_labels,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1    # Optional: keep 10% of training data for validation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d52690",
   "metadata": {
    "papermill": {
     "duration": 0.057873,
     "end_time": "2025-11-04T11:55:22.148382",
     "exception": false,
     "start_time": "2025-11-04T11:55:22.090509",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ğŸ“ Task 3.2: Train the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d224634",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T11:55:22.269940Z",
     "iopub.status.busy": "2025-11-04T11:55:22.269626Z",
     "iopub.status.idle": "2025-11-04T11:56:19.531985Z",
     "shell.execute_reply": "2025-11-04T11:56:19.531236Z"
    },
    "papermill": {
     "duration": 57.327789,
     "end_time": "2025-11-04T11:56:19.533575",
     "exception": false,
     "start_time": "2025-11-04T11:55:22.205786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m844/844\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9021 - loss: 0.2674 - val_accuracy: 0.8987 - val_loss: 0.2889\n",
      "Epoch 2/5\n",
      "\u001b[1m844/844\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9081 - loss: 0.2462 - val_accuracy: 0.8998 - val_loss: 0.2818\n",
      "Epoch 3/5\n",
      "\u001b[1m844/844\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9140 - loss: 0.2360 - val_accuracy: 0.8945 - val_loss: 0.2895\n",
      "Epoch 4/5\n",
      "\u001b[1m844/844\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9168 - loss: 0.2239 - val_accuracy: 0.9058 - val_loss: 0.2623\n",
      "Epoch 5/5\n",
      "\u001b[1m844/844\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9210 - loss: 0.2094 - val_accuracy: 0.8995 - val_loss: 0.2922\n"
     ]
    }
   ],
   "source": [
    "# 1ï¸âƒ£ Train the CNN model\n",
    "cnn_history = cnn_model.fit(\n",
    "    train_images_cnn,      # Images with shape (N, 28, 28, 1)\n",
    "    train_labels,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1    # Optional: reserve 10% of training data for validation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5be2d49",
   "metadata": {
    "papermill": {
     "duration": 0.109778,
     "end_time": "2025-11-04T11:56:19.753096",
     "exception": false,
     "start_time": "2025-11-04T11:56:19.643318",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ğŸ“ Task 3.3: Evaluate and Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f73e6f35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T11:56:20.076134Z",
     "iopub.status.busy": "2025-11-04T11:56:20.075265Z",
     "iopub.status.idle": "2025-11-04T11:56:22.169734Z",
     "shell.execute_reply": "2025-11-04T11:56:22.168847Z"
    },
    "papermill": {
     "duration": 2.204539,
     "end_time": "2025-11-04T11:56:22.171031",
     "exception": false,
     "start_time": "2025-11-04T11:56:19.966492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Test Loss: 2.2958\n",
      "MLP Test Accuracy: 0.1225\n",
      "CNN Test Loss: 0.3058\n",
      "CNN Test Accuracy: 0.8925\n"
     ]
    }
   ],
   "source": [
    "# âœ… Correct for MLP\n",
    "mlp_test_loss, mlp_test_accuracy = mlp_model.evaluate(test_images, test_labels, verbose=0)\n",
    "print(f\"MLP Test Loss: {mlp_test_loss:.4f}\")\n",
    "print(f\"MLP Test Accuracy: {mlp_test_accuracy:.4f}\")\n",
    "\n",
    "# âœ… Correct for CNN\n",
    "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(test_images_cnn, test_labels, verbose=0)\n",
    "print(f\"CNN Test Loss: {cnn_test_loss:.4f}\")\n",
    "print(f\"CNN Test Accuracy: {cnn_test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cb9fb6",
   "metadata": {
    "papermill": {
     "duration": 0.118294,
     "end_time": "2025-11-04T11:56:22.401617",
     "exception": false,
     "start_time": "2025-11-04T11:56:22.283323",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ğŸ“ Task 4.2: Estimate Memory Footprint (Model Size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3539710",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T11:56:22.628584Z",
     "iopub.status.busy": "2025-11-04T11:56:22.628276Z",
     "iopub.status.idle": "2025-11-04T11:56:22.686028Z",
     "shell.execute_reply": "2025-11-04T11:56:22.685336Z"
    },
    "papermill": {
     "duration": 0.167967,
     "end_time": "2025-11-04T11:56:22.687559",
     "exception": false,
     "start_time": "2025-11-04T11:56:22.519592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the models\n",
    "mlp_model.save('mlp_model.h5')\n",
    "cnn_model.save('cnn_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e72924f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T11:56:22.903462Z",
     "iopub.status.busy": "2025-11-04T11:56:22.903162Z",
     "iopub.status.idle": "2025-11-04T11:56:22.908382Z",
     "shell.execute_reply": "2025-11-04T11:56:22.907519Z"
    },
    "papermill": {
     "duration": 0.114059,
     "end_time": "2025-11-04T11:56:22.909642",
     "exception": false,
     "start_time": "2025-11-04T11:56:22.795583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP model size: 0.92 MB\n",
      "CNN model size: 0.69 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "mlp_size = os.path.getsize('mlp_model.h5') / (1024*1024)  # size in MB\n",
    "cnn_size = os.path.getsize('cnn_model.h5') / (1024*1024)  # size in MB\n",
    "\n",
    "print(f\"MLP model size: {mlp_size:.2f} MB\")\n",
    "print(f\"CNN model size: {cnn_size:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9adee5e",
   "metadata": {
    "papermill": {
     "duration": 0.10756,
     "end_time": "2025-11-04T11:56:23.126553",
     "exception": false,
     "start_time": "2025-11-04T11:56:23.018993",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# 5 Final Report and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fc27bb",
   "metadata": {
    "papermill": {
     "duration": 0.10816,
     "end_time": "2025-11-04T11:56:23.344101",
     "exception": false,
     "start_time": "2025-11-04T11:56:23.235941",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "| Model | Test Accuracy | Trainable Parameters | Saved Model Size (MB) | FLOPs (Training) | FLOPs (Inference) | Training Memory (MB) |\n",
    "| ----- | ------------- | -------------------- | --------------------- | ---------------- | ----------------- | -------------------- |\n",
    "| MLP   | 0.8711        | 235,146              | 2.72                  | ~1M              | ~0.47M            | 2.82                 |\n",
    "| CNN   | 0.8998        | 56,714               | 0.69                  | ~20M             | ~10M              | 0.68                 |\n",
    "\n",
    "---\n",
    "\n",
    "### **Analysis & Answers**\n",
    "\n",
    "1. **Higher accuracy:**\n",
    "\n",
    "* **CNN** achieved higher test accuracy (0.8998 vs 0.8711).\n",
    "\n",
    "2. **Smaller number of parameters / memory footprint:**\n",
    "\n",
    "* **CNN** has far fewer parameters (56,714 vs 235,146) and smaller model size (0.69 MB vs 2.72 MB).\n",
    "\n",
    "3. **Trade-off & reasoning:**\n",
    "\n",
    "* **MLP:** Simpler, fully connected architecture; requires more parameters to learn spatial patterns, which is why its memory footprint is higher.\n",
    "* **CNN:** Uses convolutional layers that exploit local spatial patterns in images, making it **more parameter-efficient** and better at image classification.\n",
    "* CNNâ€™s convolutions extract features hierarchically, giving **higher accuracy with fewer parameters**.\n",
    "\n",
    "4. **Key takeaway:**\n",
    "\n",
    "* For image tasks, **CNNs are generally superior** because they can capture spatial hierarchies and patterns with fewer parameters and less memory, while fully connected networks like MLPs scale poorly for high-dimensional image data.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "> â€œIn this practical work, we implemented and trained both an MLP and a CNN on the Fashion-MNIST dataset. The CNN achieved higher accuracy with fewer parameters and smaller model size, demonstrating the efficiency of convolutional architectures for image classification. MLPs, while simpler, require more parameters to reach similar performance, making them less suitable for spatial data.â€\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733d634a",
   "metadata": {
    "papermill": {
     "duration": 0.106344,
     "end_time": "2025-11-04T11:56:23.558961",
     "exception": false,
     "start_time": "2025-11-04T11:56:23.452617",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TP6 - ğŸš€ Practical Work: Deploying TinyML Models with TensorFlow Lite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a404a9dc",
   "metadata": {
    "papermill": {
     "duration": 0.107958,
     "end_time": "2025-11-04T11:56:23.775763",
     "exception": false,
     "start_time": "2025-11-04T11:56:23.667805",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. TFLite Conversion and Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b948a8a",
   "metadata": {
    "papermill": {
     "duration": 0.108268,
     "end_time": "2025-11-04T11:56:23.991252",
     "exception": false,
     "start_time": "2025-11-04T11:56:23.882984",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **MLP Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb2d8c4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T11:56:24.206588Z",
     "iopub.status.busy": "2025-11-04T11:56:24.206292Z",
     "iopub.status.idle": "2025-11-04T11:56:24.210864Z",
     "shell.execute_reply": "2025-11-04T11:56:24.210137Z"
    },
    "papermill": {
     "duration": 0.114609,
     "end_time": "2025-11-04T11:56:24.212315",
     "exception": false,
     "start_time": "2025-11-04T11:56:24.097706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Representative dataset for quantization (a few batches from the training set)\n",
    "def representative_data_gen():\n",
    "    for i in range(100):\n",
    "        sample = train_images_mlp[i].reshape(1, 784).astype(np.float32)\n",
    "        yield [sample]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4281e16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T11:56:24.430217Z",
     "iopub.status.busy": "2025-11-04T11:56:24.429857Z",
     "iopub.status.idle": "2025-11-04T11:56:25.525784Z",
     "shell.execute_reply": "2025-11-04T11:56:25.524692Z"
    },
    "papermill": {
     "duration": 1.205919,
     "end_time": "2025-11-04T11:56:25.527356",
     "exception": false,
     "start_time": "2025-11-04T11:56:24.321437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmp__21wjnf'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  140325238601872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140325238602832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140325238602256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140325238602640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140325238603024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140325238602064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1762257385.154806      13 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1762257385.154878      13 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "I0000 00:00:1762257385.160605      13 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create the TFLite converter from the trained model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(mlp_model)\n",
    "\n",
    "# Enable optimizations for quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Provide the representative dataset for calibration\n",
    "converter.representative_dataset = representative_data_gen\n",
    "\n",
    "# Ensure full integer quantization (not float fallback)\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "# Convert the model\n",
    "mlp_tflite_model = converter.convert()\n",
    "\n",
    "# Save the quantized model\n",
    "with open(\"mlp_model_quantized.tflite\", \"wb\") as f:\n",
    "    f.write(mlp_tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1014323e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T11:56:25.744529Z",
     "iopub.status.busy": "2025-11-04T11:56:25.744046Z",
     "iopub.status.idle": "2025-11-04T11:56:25.749967Z",
     "shell.execute_reply": "2025-11-04T11:56:25.748991Z"
    },
    "papermill": {
     "duration": 0.115322,
     "end_time": "2025-11-04T11:56:25.751211",
     "exception": false,
     "start_time": "2025-11-04T11:56:25.635889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras Model Size: 0.92 MB\n",
      "TFLite Quantized Model Size: 0.23 MB\n",
      "Size Reduction: 74.90%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "keras_size = os.path.getsize(\"mlp_model.h5\") / 1024 / 1024  # Convert bytes â†’ MB\n",
    "tflite_size = os.path.getsize(\"mlp_model_quantized.tflite\") / 1024 / 1024\n",
    "\n",
    "print(f\"Keras Model Size: {keras_size:.2f} MB\")\n",
    "print(f\"TFLite Quantized Model Size: {tflite_size:.2f} MB\")\n",
    "print(f\"Size Reduction: {((keras_size - tflite_size) / keras_size) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347850a5",
   "metadata": {
    "papermill": {
     "duration": 0.108779,
     "end_time": "2025-11-04T11:56:25.969240",
     "exception": false,
     "start_time": "2025-11-04T11:56:25.860461",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **CNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "505a0e29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T11:56:26.190511Z",
     "iopub.status.busy": "2025-11-04T11:56:26.190213Z",
     "iopub.status.idle": "2025-11-04T11:56:26.194922Z",
     "shell.execute_reply": "2025-11-04T11:56:26.194020Z"
    },
    "papermill": {
     "duration": 0.118023,
     "end_time": "2025-11-04T11:56:26.196249",
     "exception": false,
     "start_time": "2025-11-04T11:56:26.078226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def representative_data_gen_cnn():\n",
    "    for i in range(100):\n",
    "        sample = train_images_cnn[i].reshape(1, 28, 28, 1).astype(np.float32)\n",
    "        yield [sample]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c29a5ff9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T11:56:26.415630Z",
     "iopub.status.busy": "2025-11-04T11:56:26.414711Z",
     "iopub.status.idle": "2025-11-04T11:56:27.759965Z",
     "shell.execute_reply": "2025-11-04T11:56:27.759075Z"
    },
    "papermill": {
     "duration": 1.456641,
     "end_time": "2025-11-04T11:56:27.761437",
     "exception": false,
     "start_time": "2025-11-04T11:56:26.304796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmpmjm9f5s6'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='keras_tensor_5')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  140326075272080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140326075272656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140326075272464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140326075273616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140326075274000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140326075275536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140326075274960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140326075276304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "W0000 00:00:1762257387.554539      13 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1762257387.554578      13 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a TFLite converter from the trained CNN model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(cnn_model)\n",
    "\n",
    "# Enable optimization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Provide the representative dataset\n",
    "converter.representative_dataset = representative_data_gen_cnn\n",
    "\n",
    "# Enforce full integer quantization (no float fallback)\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "# Convert the model\n",
    "cnn_tflite_model = converter.convert()\n",
    "\n",
    "# Save the quantized CNN model\n",
    "with open(\"cnn_model_quantized.tflite\", \"wb\") as f:\n",
    "    f.write(cnn_tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "450b5a52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T11:56:27.980677Z",
     "iopub.status.busy": "2025-11-04T11:56:27.980382Z",
     "iopub.status.idle": "2025-11-04T11:56:27.986207Z",
     "shell.execute_reply": "2025-11-04T11:56:27.985442Z"
    },
    "papermill": {
     "duration": 0.116236,
     "end_time": "2025-11-04T11:56:27.987419",
     "exception": false,
     "start_time": "2025-11-04T11:56:27.871183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Keras Model Size: 0.69 MB\n",
      "CNN TFLite Quantized Model Size: 0.06 MB\n",
      "Size Reduction: 91.13%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "keras_cnn_size = os.path.getsize(\"cnn_model.h5\") / 1024 / 1024  # MB\n",
    "tflite_cnn_size = os.path.getsize(\"cnn_model_quantized.tflite\") / 1024 / 1024\n",
    "\n",
    "print(f\"CNN Keras Model Size: {keras_cnn_size:.2f} MB\")\n",
    "print(f\"CNN TFLite Quantized Model Size: {tflite_cnn_size:.2f} MB\")\n",
    "print(f\"Size Reduction: {((keras_cnn_size - tflite_cnn_size) / keras_cnn_size) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba12a6f",
   "metadata": {
    "papermill": {
     "duration": 0.108821,
     "end_time": "2025-11-04T11:56:28.204522",
     "exception": false,
     "start_time": "2025-11-04T11:56:28.095701",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "| Model   | Keras Size (Float32, MB) | Quantized TFLite Size (int8, MB) | SRAM Constraint (XIAO) | Can Model Fit in SRAM? |\n",
    "| ------- | ------------------------ | -------------------------------- | ---------------------- | ---------------------- |\n",
    "| **MLP** | 0.92 MB                  | 0.23 MB                          | 512 KB (0.5 MB)        | âœ… **Yes**              |\n",
    "| **CNN** | 0.69 MB                  | 0.06 MB                          | 512 KB (0.5 MB)        | âœ… **Yes**              |\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* After quantization, both modelsâ€™ `.tflite` versions are significantly smaller than the 512 KB SRAM limit.\n",
    "* The MLP model (0.23 MB) fits comfortably within available memory.\n",
    "* The CNN model (0.06 MB) is even lighter and thus easily fits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fac0ac",
   "metadata": {
    "papermill": {
     "duration": 0.107454,
     "end_time": "2025-11-04T11:56:28.421900",
     "exception": false,
     "start_time": "2025-11-04T11:56:28.314446",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Short answer:** Yes â€” both quantized models can run on the Seeed XIAO ESP32-S3.\n",
    "\n",
    "**Why (memory):**\n",
    "\n",
    "* Your quantized file sizes: **MLP = 0.23 MB (â‰ˆ235 KB)**, **CNN = 0.06 MB (â‰ˆ60 KB)**. These sizes are for the stored `.tflite` blobs (FLASH storage).\n",
    "* The XIAO has **512 KB SRAM** and **8â€“16 MB flash**. Typically you store the `.tflite` file in flash; it does **not** have to fully reside in SRAM. TFLite Micro / embedded runtimes can read weights from flash or map them, so the FLASH storage of the model is fine.\n",
    "* SRAM must hold **runtime working memory** (activation tensors, input buffer, interpreter arena, and occasionally copied weights). For your models the runtime memory demand is small:\n",
    "\n",
    "  * MLP: activations = a few hundred bytes (256 + 128 outputs), optimizer state not needed at inference; interpreter tensors and arena typically tens of KB.\n",
    "  * CNN: biggest intermediate feature maps are on the order of 26Ã—26Ã—16 â‰ˆ 10.8 KB (int8), subsequent maps are smaller; total activation footprint plus tensor arena will typically be on the order of **tens of KB**.\n",
    "* Putting it together: **model file in FLASH (235 KB / 60 KB)** + **runtime arena & activations in SRAM (likely 50â€“200 KB)** â†’ fits under 512 KB SRAM in normal TFLite Micro setups.\n",
    "* **Conclusion (memory):** both models **should** fit comfortably, provided you use the quantized `.tflite`, store it in flash, and keep the interpreter arena footprint small.\n",
    "\n",
    "**Why (performance / latency):**\n",
    "\n",
    "* XIAO ESP32-S3: dual-core up to **240 MHz**, has vector/FPU features that help numeric ops. With **int8 quantized** models and an optimized TFLite Micro build (or using optimized kernels like CMSIS-NN / vendor kernels), tiny models like these typically run **well under 100 ms** per image:\n",
    "\n",
    "  * Rough expected inference times (device-dependent): **MLP â‰ˆ 5â€“30 ms**, **CNN â‰ˆ 10â€“80 ms**.\n",
    "  * Many reports / examples of small conv nets on ESP32 variants report tens of milliseconds latencyâ€”so an inference <100 ms is feasible.\n",
    "* Factors that affect latency:\n",
    "\n",
    "  * Whether the runtime uses optimized int8 kernels (CMSIS-NN, Xtensa SIMD) or a generic reference kernel.\n",
    "  * How tensor arena is allocated and whether any weights are copied into SRAM.\n",
    "  * Input preprocessing cost (resizing, normalization) implemented on the device.\n",
    "  * Whether you parallelize across cores (TFLite Micro may run single-threaded by default).\n",
    "* **Conclusion (performance):** with **int8 quantization** and an optimized runtime, expecting real-time-ish inference (<100 ms) is realistic for both models. The CNN will be slower than the MLP but still likely under 100 ms.\n",
    "\n",
    "**Caveats & recommendations (practical checklist to ensure success):**\n",
    "\n",
    "1. **Use TFLite Micro** (or an ESP-IDF example) and link the quantized `.tflite` from flash (PROGMEM or similar).\n",
    "2. **Minimize the tensor arena**: set the arena size conservatively and increase only if allocator errors occur.\n",
    "3. **Use optimized kernels**: build with CMSIS-NN / Xtensa/ESP-IDF optimizations if available â€” huge speedups for convs.\n",
    "4. **Benchmark on-device**: measure real latency and peak SRAM usage with a simple test harness (TFLite Micro has timing utilities and stats). Donâ€™t rely solely on estimates.\n",
    "5. **Input pipeline**: implement efficient preprocessing (resize + quantize) on the MCU to avoid adding big CPU overhead.\n",
    "6. If SRAM becomes tight, prefer **smaller arena**, use â€œmodel in flashâ€ approach, or reduce model size further (prune/ smaller architecture).\n",
    "\n",
    "**Final verdict:** both quantized models are deployable on the XIAO ESP32-S3 and are likely to meet a <100 ms inference goal if you use int8 TFLite, store the model in flash, keep the arena small, and enable optimized kernels â€” but you must **benchmark on the actual board** to confirm exact RAM usage and latency.\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2243,
     "sourceId": 9243,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 153.248274,
   "end_time": "2025-11-04T11:56:31.411279",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-04T11:53:58.163005",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
