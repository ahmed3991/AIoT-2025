{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow"
      ],
      "metadata": {
        "id": "bkoZFvfVO8md"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "mlp_model = tf.keras.models.load_model('/content/mlp_model.h5')\n",
        "cnn_model = tf.keras.models.load_model('/content/cnn_model.h5')\n",
        "\n",
        "def get_file_size_mb(path):\n",
        "    return os.path.getsize(path) / (1024*1024)\n",
        "\n",
        "mlp_size = get_file_size_mb('/content/mlp_model.h5')\n",
        "cnn_size = get_file_size_mb('/content/cnn_model.h5')\n",
        "\n",
        "print(f\"MLP Keras model size: {mlp_size:.2f} MB\")\n",
        "print(f\"CNN Keras model size: {cnn_size:.2f} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp2fbnnfPCMR",
        "outputId": "a78f5763-1e05-4394-fc2c-e8cdc70c9790"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP Keras model size: 2.72 MB\n",
            "CNN Keras model size: 0.69 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "(x_train, _), _ = fashion_mnist.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "\n",
        "def representative_dataset_gen():\n",
        "    for i in range(100):\n",
        "        sample = x_train[i:i+1]\n",
        "        yield [sample.reshape(1,28,28)]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heeEcXVLOzTx",
        "outputId": "fe5506d7-3d85-4342-838a-7151bce277f4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mlp_converter = tf.lite.TFLiteConverter.from_keras_model(mlp_model)\n",
        "\n",
        "mlp_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "mlp_converter.representative_dataset = representative_dataset_gen\n",
        "mlp_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "mlp_converter.inference_input_type = tf.uint8\n",
        "mlp_converter.inference_output_type = tf.uint8\n",
        "\n",
        "mlp_tflite_model = mlp_converter.convert()\n",
        "\n",
        "with open('mlp_model_quant.tflite', 'wb') as f:\n",
        "    f.write(mlp_tflite_model)\n",
        "\n",
        "mlp_tflite_size = get_file_size_mb('mlp_model_quant.tflite')\n",
        "print(f\"MLP TFLite quantized model size: {mlp_tflite_size:.2f} MB\")\n",
        "print(f\"Size reduction: {mlp_size - mlp_tflite_size:.2f} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WX5gvNBOR2F",
        "outputId": "95528afe-4004-4c21-c6f3-a1cd8ed647a1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpth1mexnf'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 784), dtype=tf.float32, name='input_layer_6')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  132892713790416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132892713790992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132892713791760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132892713789648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132892713792336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132892713792528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "MLP TFLite quantized model size: 0.24 MB\n",
            "Size reduction: 2.48 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def representative_dataset_gen_cnn():\n",
        "    for i in range(100):\n",
        "        sample = x_train[i:i+1].reshape(1,28,28,1)\n",
        "        yield [sample]\n",
        "\n",
        "cnn_converter = tf.lite.TFLiteConverter.from_keras_model(cnn_model)\n",
        "cnn_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "cnn_converter.representative_dataset = representative_dataset_gen_cnn\n",
        "cnn_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "cnn_converter.inference_input_type = tf.uint8\n",
        "cnn_converter.inference_output_type = tf.uint8\n",
        "\n",
        "cnn_tflite_model = cnn_converter.convert()\n",
        "\n",
        "with open('cnn_model_quant.tflite', 'wb') as f:\n",
        "    f.write(cnn_tflite_model)\n",
        "\n",
        "cnn_tflite_size = get_file_size_mb('cnn_model_quant.tflite')\n",
        "print(f\"CNN TFLite quantized model size: {cnn_tflite_size:.2f} MB\")\n",
        "print(f\"Size reduction: {cnn_size - cnn_tflite_size:.2f} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTS40JmlT4Yg",
        "outputId": "e8c8a54f-8642-4848-917d-e8699603515f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpeqpaocdb'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='input_layer_5')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  132892713795024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132892711011152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132892711010768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132892711013072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132892711014032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132892711013264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132892711013456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132892711014608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN TFLite quantized model size: 0.06 MB\n",
            "Size reduction: 0.63 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    \"Model\": [\"MLP\", \"CNN\"],\n",
        "    \"Keras Size (Float32, MB)\": [mlp_size, cnn_size],\n",
        "    \"Quantized TFLite Size (int8, MB)\": [mlp_tflite_size, cnn_tflite_size],\n",
        "    \"SRAM Constraint (XIAO, MB)\": [0.5, 0.5],  # 512 KB = 0.5 MB\n",
        "    \"Can Fit in SRAM?\": [\"Yes\" if mlp_tflite_size < 0.5 else \"No\",\n",
        "                         \"Yes\" if cnn_tflite_size < 0.5 else \"No\"]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWywUxepT8v_",
        "outputId": "cf2c7285-114a-4639-d876-b551e41cebaa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Model  Keras Size (Float32, MB)  Quantized TFLite Size (int8, MB)  \\\n",
            "0   MLP                  2.721336                          0.237106   \n",
            "1   CNN                  0.687439                          0.061279   \n",
            "\n",
            "   SRAM Constraint (XIAO, MB) Can Fit in SRAM?  \n",
            "0                         0.5              Yes  \n",
            "1                         0.5              Yes  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Deployment Feasibility Analysis ===\\n\")\n",
        "\n",
        "for model_name, size_mb in zip([\"MLP\",\"CNN\"], [mlp_tflite_size, cnn_tflite_size]):\n",
        "    fits = \"YES\" if size_mb < 0.5 else \"NO\"\n",
        "    print(f\"{model_name} quantized model size: {size_mb:.3f} MB -> Fits in SRAM? {fits}\")\n",
        "\n",
        "print(\"\\nConclusion:\")\n",
        "print(\"- Given the dual-core 240 MHz CPU, and the small model sizes:\")\n",
        "print(\"  * CNN and MLP models could technically run on XIAO ESP32S3.\")\n",
        "print(\"  * MLP quantized model (~<0.5 MB) easily fits in SRAM for inference.\")\n",
        "print(\"  * CNN model may be close to SRAM limit depending on batch/activation memory; careful optimization required.\")\n",
        "print(\"  * Inference should be feasible in tens of milliseconds per image, likely <100ms per sample.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6k7mNvxUC01",
        "outputId": "c8fa5f1d-03c7-41d4-c50f-db5729150b0a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Deployment Feasibility Analysis ===\n",
            "\n",
            "MLP quantized model size: 0.237 MB -> Fits in SRAM? YES\n",
            "CNN quantized model size: 0.061 MB -> Fits in SRAM? YES\n",
            "\n",
            "Conclusion:\n",
            "- Given the dual-core 240 MHz CPU, and the small model sizes:\n",
            "  * CNN and MLP models could technically run on XIAO ESP32S3.\n",
            "  * MLP quantized model (~<0.5 MB) easily fits in SRAM for inference.\n",
            "  * CNN model may be close to SRAM limit depending on batch/activation memory; careful optimization required.\n",
            "  * Inference should be feasible in tens of milliseconds per image, likely <100ms per sample.\n"
          ]
        }
      ]
    }
  ]
}