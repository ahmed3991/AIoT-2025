# -*- coding: utf-8 -*-
"""TP0506AIIOT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d80Sw8I8C0hYpHxFk1mJ3ivyeDMlUVqQ

# Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ¦Ø© ÙˆØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Fashion-MNIST
"""

# ğŸ—ï¸ 1.1 Setup and Data Loading

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø©
import tensorflow as tf
from tensorflow import keras
from keras.datasets import fashion_mnist
from keras.models import Sequential
from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (ØªÙÙ‚Ø³Ù… ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¥Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø±)
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

# ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ù†Ø·Ø§Ù‚ [0, 1]
x_train = x_train / 255.0
x_test = x_test / 255.0

# âš™ï¸ Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ÙƒÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø­Ø³Ø¨ ÙƒÙ„ Ù†Ù…ÙˆØ°Ø¬

# Ù„Ù„Ù€ MLP â†’ Ù„Ø§ Ø­Ø§Ø¬Ø© Ù„Ø¥Ø¶Ø§ÙØ© Ù‚Ù†Ø§Ø©ØŒ ÙÙ‚Ø· Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø´ÙƒÙ„ (N, 28, 28)
x_train_mlp = x_train.reshape(-1, 28, 28)
x_test_mlp = x_test.reshape(-1, 28, 28)

# Ù„Ù„Ù€ CNN â†’ Ø¥Ø¶Ø§ÙØ© Ø¨Ø¹Ø¯ Ø§Ù„Ù‚Ù†Ø§Ø© (1)
x_train_cnn = x_train.reshape(-1, 28, 28, 1)
x_test_cnn = x_test.reshape(-1, 28, 28, 1)

# Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ø£Ø´ÙƒØ§Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„ØªØ­Ù‚Ù‚
print("Shape for MLP input:", x_train_mlp.shape)
print("Shape for CNN input:", x_train_cnn.shape)

"""# Task 2.1: Ø¥Ù†Ø´Ø§Ø¡ ÙˆØªØ¬Ù…ÙŠØ¹ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù€ MLP"""

# ğŸ§  2.1 Implement and Compile the MLP Model

# ØªØ¹Ø±ÙŠÙ Ù†Ù…ÙˆØ°Ø¬ MLP Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Keras Sequential API
mlp_model = Sequential([
    Flatten(input_shape=(28, 28)),         # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ù…ØªØ¬Ù‡ 784 Ø¹Ù†ØµØ±
    Dense(256, activation='relu'),         # Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ù…Ø®ÙÙŠØ© Ø§Ù„Ø£ÙˆÙ„Ù‰
    Dense(128, activation='relu'),         # Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ù…Ø®ÙÙŠØ© Ø§Ù„Ø«Ø§Ù†ÙŠØ©
    Dense(10, activation='softmax')        # Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© (ØªØµÙ†ÙŠÙ Ø¥Ù„Ù‰ 10 ÙØ¦Ø§Øª)
])

# ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (compile)
mlp_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Ø¹Ø±Ø¶ Ù…Ù„Ø®Øµ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
mlp_model.summary()

"""# Task 2.2: Ø¥Ù†Ø´Ø§Ø¡ ÙˆØªØ¬Ù…ÙŠØ¹ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù€ CNN"""

#  2.2 Implement and Compile the CNN Model

cnn_model = Sequential([
    # Ø§Ù„ÙƒØªÙ„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: Convolution + MaxPooling
    Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D((2, 2)),

    # Ø§Ù„ÙƒØªÙ„Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: Convolution + MaxPooling
    Conv2D(32, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),

    # Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ù„Ù„ØªØµÙ†ÙŠÙ
    Flatten(),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])

# ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
cnn_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Ø¹Ø±Ø¶ Ù…Ù„Ø®Øµ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
cnn_model.summary()

"""# Task 3.1: ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù€ MLP"""

# ğŸ§  ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù€ MLP
history_mlp = mlp_model.fit(
    x_train_mlp, y_train,
    epochs=5,
    batch_size=64,
    validation_split=0.1,   # Ù†Ø®ØµØµ 10% Ù…Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù„Ù„ØªØ­Ù‚Ù‚ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    verbose=2
)

"""# Task 3.2: ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù€ CNN"""

#  ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù€ CNN
history_cnn = cnn_model.fit(
    x_train_cnn, y_train,
    epochs=5,
    batch_size=64,
    validation_split=0.1,
    verbose=2
)

"""# Task 3.3: ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ÙŠÙ† Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±"""

# ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ÙŠÙ† Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
mlp_test_loss, mlp_test_acc = mlp_model.evaluate(x_test_mlp, y_test, verbose=0)
cnn_test_loss, cnn_test_acc = cnn_model.evaluate(x_test_cnn, y_test, verbose=0)

# Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
print("ğŸ§  MLP Model Performance:")
print(f"Test Accuracy: {mlp_test_acc:.4f}")
print(f"Test Loss: {mlp_test_loss:.4f}\n")

print("ğŸ§© CNN Model Performance:")
print(f"Test Accuracy: {cnn_test_acc:.4f}")
print(f"Test Loss: {cnn_test_loss:.4f}")

"""# Task 4.1: Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨"""

# Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨
mlp_params = mlp_model.count_params()
cnn_params = cnn_model.count_params()

print(f"ğŸ§  MLP Trainable Parameters: {mlp_params:,}")
print(f"ğŸ§© CNN Trainable Parameters: {cnn_params:,}")

"""ğŸ”¹ ØªÙØ³ÙŠØ± Ù†Ù…ÙˆØ°Ø¬ÙŠ Ù„Ù„Ù†ØªØ§Ø¦Ø¬:

MLP: â‰ˆ 266,634 Ù…Ø¹Ø§Ù…Ù„ (parameters)

CNN: â‰ˆ 56,714 Ù…Ø¹Ø§Ù…Ù„
âœ Ù†Ù„Ø§Ø­Ø¸ Ø£Ù† CNN ÙŠØ³ØªØ®Ø¯Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø£Ù‚Ù„ ÙˆÙ„ÙƒÙ†Ù‡ ÙŠØ­Ù‚Ù‚ Ø£Ø¯Ø§Ø¡ Ø£ÙØ¶Ù„ ØºØ§Ù„Ø¨Ù‹Ø§ â€” Ù„Ø£Ù†Ù‡ ÙŠØ³ØªÙÙŠØ¯ Ù…Ù† Ø§Ù„ØªØ´Ø§Ø±ÙƒÙŠØ© ÙÙŠ Ø§Ù„Ø£ÙˆØ²Ø§Ù† (weight sharing).

# Task 4.2: ØªÙ‚Ø¯ÙŠØ± Ø­Ø¬Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Memory Footprint)
"""

import os

# Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
mlp_model.save('mlp_model.h5')
cnn_model.save('cnn_model.h5')

# Ø­Ø³Ø§Ø¨ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„ÙØ§Øª Ø¨Ø§Ù„Ù…ÙŠØºØ§Ø¨Ø§ÙŠØª
mlp_size = os.path.getsize('mlp_model.h5') / (1024 * 1024)
cnn_size = os.path.getsize('cnn_model.h5') / (1024 * 1024)

print(f"ğŸ§  MLP Model Size: {mlp_size:.2f} MB")
print(f"ğŸ§© CNN Model Size: {cnn_size:.2f} MB")

"""ğŸ”¹ ØªÙØ³ÙŠØ± Ù†Ù…ÙˆØ°Ø¬ÙŠ Ù„Ù„Ù†ØªØ§Ø¦Ø¬:

mlp_model.h5 â‰ˆ 1.1 MB

cnn_model.h5 â‰ˆ 0.25 MB

ğŸ’¡ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬:
Ø§Ù„Ù€ CNN Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø±ØºÙ… Ø£Ø¯Ø§Ø¦Ù‡ Ø§Ù„Ø£ÙØ¶Ù„ØŒ Ø¨ÙØ¶Ù„ Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø§Ù„ØªÙØ§Ù Ø§Ù„ØµØºÙŠØ±Ø© Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø© ÙÙŠ Ø§Ù„Ù€ MLP.

ğŸ“ Task 4.3: ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø­Ø³Ø§Ø¨ÙŠØ© (FLOPs & Memory for Training)

Ù‡Ø°Ù‡ Ø§Ù„ØªÙ‚Ø¯ÙŠØ±Ø§Øª ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙ‚Ø±ÙŠØ¨ÙŠØŒ ÙˆØ³Ø£ÙˆØ¶Ø­ Ù„Ùƒ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† Ø­Ø³Ø§Ø¨Ù‡Ø§ Ø¨Ø´ÙƒÙ„ ØªÙ‚Ø±ÙŠØ¨ÙŠ Ø¯ÙˆÙ† Ø£Ø¯ÙˆØ§Øª Ø®Ø§Ø±Ø¬ÙŠØ©.

ğŸ§® Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª (FLOPs)

ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø© Ù…Ø«Ù„ tf.profiler.experimental Ø£Ùˆ Ù…ÙƒØªØ¨Ø© keras-flopsØŒ Ù„ÙƒÙ† ÙÙŠ ÙƒÙˆÙ„Ø§Ø¨ ÙŠÙ…ÙƒÙ†Ù†Ø§ ØªÙ‚Ø¯ÙŠØ±Ù‡Ø§ ØªÙ‚Ø±ÙŠØ¨ÙŠÙ‹Ø§:

MLP:

ÙƒÙ„ Ø·Ø¨Ù‚Ø© Dense Ø¨Ù€
ğ‘›
ğ‘–
ğ‘›
Ã—
ğ‘›
ğ‘œ
ğ‘¢
ğ‘¡
n
in
	â€‹

Ã—n
out
	â€‹

 Ø¹Ù…Ù„ÙŠØ© ØªÙ‚Ø±ÙŠØ¨Ù‹Ø§.

Ù…Ø«Ø§Ù„:

784Ã—256 + 256Ã—128 + 128Ã—10 â‰ˆ 226k Ø¹Ù…Ù„ÙŠØ§Øª ÙÙŠ Ø§Ù„Ù€ forward pass.

Ø¨Ø§Ù„ØªØ§Ù„ÙŠØŒ ØªÙ‚Ø±ÙŠØ¨Ù‹Ø§ 0.23 Ù…Ù„ÙŠÙˆÙ† FLOPs (forward pass).

Ù…Ø¹ Ø§Ù„Ù€ backward pass (Ø§Ù„ØªØ¯Ø±ÙŠØ¨) â‰ˆ 2Ã— â‡’ â‰ˆ 0.46 Ù…Ù„ÙŠÙˆÙ† FLOPs.

CNN:

Convolution Ø¹Ù…Ù„ÙŠØ© Ø£Ø«Ù‚Ù„ØŒ ØªÙØ­Ø³Ø¨ ØªÙ‚Ø±ÙŠØ¨Ù‹Ø§ ÙƒØ§Ù„ØªØ§Ù„ÙŠ:

ğ¹
ğ¿
ğ‘‚
ğ‘ƒ
ğ‘ 
=
(
ğ¾
2
Ã—
ğ¶
ğ‘–
ğ‘›
Ã—
ğ»
ğ‘œ
ğ‘¢
ğ‘¡
Ã—
ğ‘Š
ğ‘œ
ğ‘¢
ğ‘¡
Ã—
ğ¶
ğ‘œ
ğ‘¢
ğ‘¡
)
FLOPs=(K
2
Ã—C
in
	â€‹

Ã—H
out
	â€‹

Ã—W
out
	â€‹

Ã—C
out
	â€‹

)

Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø·Ø¨Ù‚Ø§Øª Ù„Ø¯ÙŠÙƒ:

Conv1 â‰ˆ 300k FLOPs

Conv2 â‰ˆ 600k FLOPs

Dense layers â‰ˆ 60k FLOPs
âœ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ â‰ˆ 1 Ù…Ù„ÙŠÙˆÙ† FLOPs (forward)
âœ 2 Ù…Ù„ÙŠÙˆÙ† FLOPs (forward + backward) Ù„Ù„ØªØ¯Ø±ÙŠØ¨.

ğŸ”¹ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„ØªÙ‚Ø±ÙŠØ¨ÙŠØ©:

Model	FLOPs (Forward)	FLOPs (Train Step)
MLP	~0.23M	~0.46M
CNN	~1.0M	~2.0M

ğŸ’¾ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨

ÙŠØªØ¶Ù…Ù†:

Ø§Ù„Ø£ÙˆØ²Ø§Ù† (Parameters)

Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø³Ù† (Optimizer State)

Ø§Ù„Ù…ØªØ¯Ø±Ø¬Ø§Øª (Gradients)

ÙƒÙ„ Ù…Ø¹Ø§Ù…Ù„ ÙŠØ³ØªØ®Ø¯Ù… ØªÙ‚Ø±ÙŠØ¨Ù‹Ø§ 4 bytes (float32).
Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ â‰ˆ
params
Ã—
3
Ã—
4
paramsÃ—3Ã—4 bytes.
"""

def estimate_training_memory(params):
    bytes_per_param = 4
    multiplier = 3  # parameters + gradients + optimizer state
    total_bytes = params * bytes_per_param * multiplier
    return total_bytes / (1024 * 1024)  # Ø¨Ø§Ù„Ù…ÙŠØºØ§Ø¨Ø§ÙŠØª

mlp_mem = estimate_training_memory(mlp_params)
cnn_mem = estimate_training_memory(cnn_params)

print(f"ğŸ§  MLP Estimated Training Memory: {mlp_mem:.2f} MB")
print(f"ğŸ§© CNN Estimated Training Memory: {cnn_mem:.2f} MB")

"""ğŸ“ Task 5.1 â€“ Summary Table
Model	Test Accuracy	Trainable Parameters	Saved Model Size (MB)	FLOPs (Training)	FLOPs (Inference)	Training Memory (MB)
ğŸ§  MLP	~0.88	266,634	~1.10 MB	~0.46M	~0.23M	~3.05 MB
ğŸ§© CNN	~0.92	56,714	~0.25 MB	~2.00M	~1.00M	~0.65 MB


ğŸ’¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
1ï¸âƒ£ Ø£ÙŠ Ù†Ù…ÙˆØ°Ø¬ Ø­Ù‚Ù‚ Ø¯Ù‚Ø© Ø£Ø¹Ù„Ù‰ØŸ

âœ… Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù€ CNN Ø­Ù‚Ù‚ Ø¯Ù‚Ø© Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø¹Ù„Ù‰ (~92%) Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨Ù€ MLP (~88%).
ÙˆØ°Ù„Ùƒ Ù„Ø£Ù† Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø§Ù„ØªÙØ§ÙÙŠØ© (Convolutional Networks) Ù‚Ø§Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„Ø³Ù…Ø§Øª Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ© Spatial Features Ù…Ù† Ø§Ù„ØµÙˆØ± Ø¨Ø´ÙƒÙ„ ÙØ¹Ø§Ù„ Ø¨ÙØ¶Ù„ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø§Ù„ØªÙØ§ÙÙŠØ© (Conv2D).

2ï¸âƒ£ Ø£ÙŠ Ù†Ù…ÙˆØ°Ø¬ ÙŠØ³ØªØ®Ø¯Ù… Ø°Ø§ÙƒØ±Ø© ÙˆÙ…Ø¹Ø§Ù…Ù„Ø§Øª Ø£Ù‚Ù„ØŸ

âœ… Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù€ CNN ÙŠØ³ØªØ®Ø¯Ù… Ø¹Ø¯Ø¯ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø£Ù‚Ù„ (â‰ˆ 56K ÙÙ‚Ø·) Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨Ù€ MLP (â‰ˆ 266K)ØŒ
ÙƒÙ…Ø§ Ø£Ù† Ø­Ø¬Ù… Ù…Ù„Ù Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ CNN Ø£ØµØºØ± (~0.25 MB) Ù…Ù‚Ø§Ø¨Ù„ (~1.1 MB) Ù„Ù„Ù€ MLP.
ÙˆÙ‡Ø°Ø§ ÙŠØ¬Ø¹Ù„Ù‡ Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© Ù…Ù† Ù†Ø§Ø­ÙŠØ© Ø§Ù„ØªØ®Ø²ÙŠÙ† ÙˆØ§Ù„Ù†Ø´Ø± (deployment).

3ï¸âƒ£ Ù…Ø§ Ù‡Ùˆ Ø§Ù„ØªÙˆØ§Ø²Ù† (Trade-off) Ø¨ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ÙŠÙ†ØŸ
Ø¬Ø§Ù†Ø¨ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©	MLP	CNN
Ø§Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ø­Ø³Ø§Ø¨ÙŠØ© (FLOPs)	Ø£Ø³Ø±Ø¹ ÙˆØ£Ø®Ù ÙÙŠ Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª	Ø£Ø¨Ø·Ø£ Ø¨Ø³Ø¨Ø¨ Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø§Ù„ØªÙØ§Ù
Ø§Ù„Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±ÙŠ	Ø£Ø¹Ù„Ù‰ Ø¨Ø³Ø¨Ø¨ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„ÙƒØ«ÙŠÙØ©	Ø£Ù‚Ù„ ÙˆØ£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø©
Ø§Ù„Ø¯Ù‚Ø© ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ±	Ø£Ù‚Ù„ØŒ Ù„Ø£Ù†Ù‡ ÙŠØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ© Ù„Ù„ØµÙˆØ±Ø©	Ø£Ø¹Ù„Ù‰ØŒ Ù„Ø£Ù†Ù‡ ÙŠØªØ¹Ù„Ù… Ø§Ù„Ø³Ù…Ø§Øª Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ©
Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ù†Ø§Ø³Ø¨	Ø¬ÙŠØ¯ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ© Ø£Ùˆ Ø§Ù„Ù…ÙˆØ¬Ù‡Ø© Ø¹Ø¯Ø¯ÙŠÙ‹Ø§	Ù…Ù…ØªØ§Ø² Ù„Ù„ØµÙˆØ± ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø±Ø¦ÙŠØ©
ğŸ§  Ù„Ù…Ø§Ø°Ø§ CNN Ø£ÙØ¶Ù„ ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ±ØŸ

Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ© Ù„Ù„ØµÙˆØ±Ø©:
Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ù€ Convolution ØªØ³ØªÙÙŠØ¯ Ù…Ù† Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…ÙƒØ§Ù†ÙŠ Ù„Ù„Ø¨ÙƒØ³Ù„Ø§ØªØŒ Ø¨Ø¹ÙƒØ³ Ø§Ù„Ù€ MLP Ø§Ù„Ø°ÙŠ ÙŠÙÙ‚Ø¯ Ù‡Ø°Ø§ Ø§Ù„ØªØ±ØªÙŠØ¨ Ø¹Ù†Ø¯ "ØªØ³Ø·ÙŠØ­" Ø§Ù„ØµÙˆØ±Ø©.

Ù…Ø´Ø§Ø±ÙƒØ© Ø§Ù„Ø£ÙˆØ²Ø§Ù† (Weight Sharing):
Ù†ÙØ³ Ø§Ù„ÙÙ„ØªØ± (kernel) ÙŠÙØ³ØªØ®Ø¯Ù… Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„ØµÙˆØ±Ø©ØŒ Ù…Ù…Ø§ ÙŠÙ‚Ù„Ù„ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ± ÙˆÙŠØ²ÙŠØ¯ Ø§Ù„ÙƒÙØ§Ø¡Ø©.

Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³Ù…Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª:
Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø§Ù„ØªÙØ§ÙÙŠØ© ØªØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¨Ø³ÙŠØ·Ø© (Ù…Ø«Ù„ Ø§Ù„Ø­ÙˆØ§Ù) Ø¥Ù„Ù‰ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø© (Ù…Ø«Ù„ Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„ÙƒØ§Ù…Ù„) ØªØ¯Ø±ÙŠØ¬ÙŠÙ‹Ø§.

Ù‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù…ÙŠÙ… Ø§Ù„Ø¹Ø§Ù„ÙŠØ©:
Ù„Ø£Ù† Ø§Ù„Ø´Ø¨ÙƒØ© ØªØªØ¹Ù„Ù… Ø§Ù„Ù…ÙŠØ²Ø§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ØŒ ÙÙ‡ÙŠ Ø£Ù‚Ù„ Ø¹Ø±Ø¶Ø© Ù„ÙØ±Ø· Ø§Ù„ØªØ®ØµÙŠØµ (overfitting) Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¨ØµØ±ÙŠØ©.

ğŸ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ

Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙ…ÙŠ ÙˆØ§Ù„Ù†ÙˆØ¹ÙŠ:

ğŸ”¹ Ù†Ù…ÙˆØ°Ø¬ CNN Ù‡Ùˆ Ø§Ù„Ø£Ù†Ø³Ø¨ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± ÙÙŠ Fashion-MNIST.
ğŸ”¹ Ø¨ÙŠÙ†Ù…Ø§ Ø§Ù„Ù€ MLP Ø£Ø¨Ø³Ø· ÙˆØ£Ø³Ø±Ø¹ØŒ Ø¥Ù„Ø§ Ø£Ù†Ù‡ ØºÙŠØ± ÙƒØ§ÙÙ Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø© Ø¨ÙŠÙ† Ø§Ù„Ø¨ÙƒØ³Ù„Ø§Øª.
ğŸ”¹ Ø¨Ø§Ù„ØªØ§Ù„ÙŠØŒ ÙŠÙˆØµÙ‰ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… CNN ÙÙŠ Ù…Ù‡Ø§Ù… Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©ØŒ
Ø®ØµÙˆØµÙ‹Ø§ Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† Ø§Ù„ØµÙˆØ± Ù…Ø¯Ø®Ù„Ø© Ø£Ø³Ø§Ø³ÙŠØ©ØŒ ÙˆÙŠÙƒÙˆÙ† Ø§Ù„Ù‡Ø¯Ù Ù‡Ùˆ Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© ÙˆÙƒÙØ§Ø¡Ø© ÙÙŠ Ø§Ù„ØªØ¹Ù„Ù….
"""

# =========================
# Fashion-MNIST Final Report (PDF)
# =========================

import os
import math
from datetime import datetime

# 1) Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ù„Ø§Ù„ØªÙ‚Ø§Ø· Ø§Ù„Ù‚ÙŠÙ… Ù…Ù† Ø§Ù„Ø¬Ù„Ø³Ø© Ø¥Ù† ÙˆÙØ¬Ø¯Øª
def safe_get(varname, default=None):
    return globals().get(varname, default)

# Ø§Ù„ØªÙ‚Ø§Ø· Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
mlp_model = safe_get('mlp_model')
cnn_model = safe_get('cnn_model')

# Ø§Ù„ØªÙ‚Ø§Ø· Ø¯Ø§ØªØ§ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± (Ù‚Ø¯ ØªÙƒÙˆÙ† Ù…ÙˆØ¬ÙˆØ¯Ø© Ù…Ù† Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©)
x_test_mlp = safe_get('x_test_mlp')
x_test_cnn = safe_get('x_test_cnn')
y_test     = safe_get('y_test')

# Ø§Ù„ØªÙ‚Ø§Ø· Ù†ØªØ§Ø¦Ø¬ Ø³Ø§Ø¨Ù‚Ø© Ø¥Ù† ÙˆÙØ¬Ø¯Øª
mlp_test_loss = safe_get('mlp_test_loss')
mlp_test_acc  = safe_get('mlp_test_acc')
cnn_test_loss = safe_get('cnn_test_loss')
cnn_test_acc  = safe_get('cnn_test_acc')

# 2) Ø­Ø³Ø§Ø¨/Ø¬Ù„Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
mlp_params = mlp_model.count_params() if mlp_model else None
cnn_params = cnn_model.count_params() if cnn_model else None

# 3) ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„Ø®Ø³Ø§Ø±Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…ÙˆØ¬ÙˆØ¯Ø© ÙˆÙ„Ù… ØªÙƒÙ† Ø§Ù„Ù‚ÙŠÙ… Ù…Ø­ÙÙˆØ¸Ø©
def try_evaluate(model, x, y):
    try:
        if (model is not None) and (x is not None) and (y is not None):
            loss, acc = model.evaluate(x, y, verbose=0)
            return float(loss), float(acc)
    except Exception as e:
        pass
    return None, None

if mlp_test_acc is None or mlp_test_loss is None:
    l, a = try_evaluate(mlp_model, x_test_mlp, y_test)
    mlp_test_loss = mlp_test_loss if mlp_test_loss is not None else l
    mlp_test_acc  = mlp_test_acc  if mlp_test_acc  is not None else a

if cnn_test_acc is None or cnn_test_loss is None:
    l, a = try_evaluate(cnn_model, x_test_cnn, y_test)
    cnn_test_loss = cnn_test_loss if cnn_test_loss is not None else l
    cnn_test_acc  = cnn_test_acc  if cnn_test_acc  is not None else a

# 4) Ø£Ø­Ø¬Ø§Ù… Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø© (.h5)
def file_size_mb(path):
    try:
        return os.path.getsize(path) / (1024*1024)
    except:
        return None

# Ù„Ùˆ Ù„Ù… ØªÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø©ØŒ Ù„Ø§ Ù…Ø´ÙƒÙ„Ø© â€” Ø³Ù†Ø¹Ø±Ø¶ N/A
mlp_h5 = 'mlp_model.h5'
cnn_h5 = 'cnn_model.h5'
mlp_size = file_size_mb(mlp_h5)
cnn_size = file_size_mb(cnn_h5)

# 5) ØªÙ‚Ø¯ÙŠØ± FLOPs (ØªÙ‚Ø±ÙŠØ¨ÙŠ Ø¬Ø¯Ù‹Ø§) + Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
# Ù…Ù„Ø§Ø­Ø¸Ø©: Ù‡Ø°Ù‡ ØªÙ‚Ø¯ÙŠØ±Ø§Øª Ù…Ø¨Ø³Ø·Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ
def estimate_training_memory_mb(params):
    # float32: 4 bytes Ù„ÙƒÙ„ Ù…Ø¹Ø§Ù…Ù„
    # Parameters + Gradients + Optimizer state â‰ˆ 3x
    if params is None: return None
    return (params * 4 * 3) / (1024*1024)

# ØªÙ‚Ø¯ÙŠØ± FLOPs (ØªÙ‚Ø±ÙŠØ¨ÙŠ) â€” ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø­Ø¯Ø¯ Ù„Ø¯ÙŠÙ†Ø§:
# Ù…Ù† Ø§Ù„Ø´Ø±Ø­ Ø§Ù„Ø³Ø§Ø¨Ù‚: (Ù‚ÙŠÙ… Ù…Ø±Ø¬Ø¹ÙŠØ© ØªÙ‚Ø±ÙŠØ¨ÙŠØ©)
mlp_flops_inf  = 0.23e6  # ~0.23M
mlp_flops_train = 0.46e6 # ~0.46M
cnn_flops_inf  = 1.00e6  # ~1.0M
cnn_flops_train = 2.00e6 # ~2.0M

mlp_mem_train = estimate_training_memory_mb(mlp_params)
cnn_mem_train = estimate_training_memory_mb(cnn_params)

# 6) ØªØ¬Ù‡ÙŠØ² Ø¬Ø¯ÙˆÙ„ Ø§Ù„ØªÙ‚Ø±ÙŠØ± (Ù…Ø¹ Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù†ØµÙˆØµ Ù…Ù†Ø³Ù‚Ø©)
def fmt(v, fmt_str="{:.4f}"):
    if v is None: return "N/A"
    try:
        return fmt_str.format(v)
    except:
        return str(v)

def fmt_int(v):
    if v is None: return "N/A"
    return f"{int(v):,}"

def fmt_mb(v):
    if v is None: return "N/A"
    return f"{v:.2f} MB"

def fmt_flops(v):
    if v is None: return "N/A"
    # Ù†Ø¹Ø±Ø¶ Ø¨Ø§Ù„Ù…Ù„Ø§ÙŠÙŠÙ† Ù„Ù„Ø§Ø®ØªØµØ§Ø±
    return f"{v/1e6:.2f}M"

report_rows = [
    {
        "Model": "MLP",
        "Test Accuracy": fmt(mlp_test_acc),
        "Trainable Parameters": fmt_int(mlp_params),
        "Saved Model Size (MB)": fmt_mb(mlp_size),
        "FLOPs (Training)": fmt_flops(mlp_flops_train),
        "FLOPs (Inference)": fmt_flops(mlp_flops_inf),
        "Training Memory (MB)": fmt(mlp_mem_train, "{:.2f}")
    },
    {
        "Model": "CNN",
        "Test Accuracy": fmt(cnn_test_acc),
        "Trainable Parameters": fmt_int(cnn_params),
        "Saved Model Size (MB)": fmt_mb(cnn_size),
        "FLOPs (Training)": fmt_flops(cnn_flops_train),
        "FLOPs (Inference)": fmt_flops(cnn_flops_inf),
        "Training Memory (MB)": fmt(cnn_mem_train, "{:.2f}")
    }
]

# 7) Ø¥Ù†Ø´Ø§Ø¡ CSV Ù„Ù„Ø¬Ø¯ÙˆÙ„ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ Ù„Ù„Ø¹Ø±Ø¶ ÙˆØ§Ù„Ù…Ø´Ø§Ø±ÙƒØ©)
import csv
csv_path = "fashionmnist_summary.csv"
with open(csv_path, "w", newline="", encoding="utf-8") as f:
    writer = csv.DictWriter(f, fieldnames=list(report_rows[0].keys()))
    writer.writeheader()
    for r in report_rows:
        writer.writerow(r)

# 8) Ø¥Ù†Ø´Ø§Ø¡ PDF Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… reportlab
!pip -q install reportlab >/dev/null

from reportlab.lib.pagesizes import A4
from reportlab.pdfgen import canvas
from reportlab.lib import colors
from reportlab.lib.units import cm
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle
from reportlab.lib.styles import getSampleStyleSheet

pdf_path = "FashionMNIST_Report.pdf"
doc = SimpleDocTemplate(pdf_path, pagesize=A4, rightMargin=2*cm, leftMargin=2*cm, topMargin=1.5*cm, bottomMargin=1.5*cm)
styles = getSampleStyleSheet()
story = []

title = Paragraph("<b>Fashion-MNIST Image Classification â€“ Final Report</b>", styles["Title"])
subtitle = Paragraph(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", styles["Normal"])
story += [title, subtitle, Spacer(1, 12)]

# Ù†Ø¨Ø°Ø© Ù‚ØµÙŠØ±Ø©
intro = """
<b>Overview:</b> This report summarizes training and evaluation of two architectures (MLP & CNN) on the Fashion-MNIST dataset using TensorFlow/Keras. It includes accuracy, model complexity (parameters), storage footprint, and rough estimates of FLOPs and training memory.
"""
story += [Paragraph(intro, styles["BodyText"]), Spacer(1, 12)]

# Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ù„Ø®Øµ
table_data = [["Model","Test Accuracy","Trainable Parameters","Saved Model Size (MB)","FLOPs (Training)","FLOPs (Inference)","Training Memory (MB)"]]
for r in report_rows:
    table_data.append([r[k] for k in table_data[0]])

tbl = Table(table_data, hAlign='LEFT')
tbl.setStyle(TableStyle([
    ("BACKGROUND", (0,0), (-1,0), colors.lightgrey),
    ("TEXTCOLOR", (0,0), (-1,0), colors.black),
    ("ALIGN", (0,0), (-1,-1), "CENTER"),
    ("FONTNAME", (0,0), (-1,0), "Helvetica-Bold"),
    ("BOTTOMPADDING", (0,0), (-1,0), 8),
    ("GRID", (0,0), (-1,-1), 0.5, colors.grey),
]))
story += [tbl, Spacer(1, 16)]

# Ø§Ù„Ø®Ù„Ø§ØµØ©
conclusion = """
<b>Conclusion:</b><br/>
â€¢ The CNN achieved higher test accuracy, thanks to spatial feature extraction via convolution and weight sharing, while keeping parameter count and saved size lower than the MLP.<br/>
â€¢ The MLP is simpler and has fewer FLOPs per inference in this setup, but it discards spatial structure by flattening, which typically limits image classification performance.<br/>
â€¢ For image tasks, CNNs are generally superior due to learning hierarchical, translation-aware features with fewer parameters.
"""
story += [Paragraph(conclusion, styles["BodyText"]), Spacer(1, 12)]

# ØªÙØ§ØµÙŠÙ„ Ø¥Ø¶Ø§ÙÙŠØ©/Ù…Ù„Ø§Ø­Ø¸Ø§Øª
notes = """
<b>Notes:</b> Reported FLOPs are rough academic estimates for this specific architecture. Actual runtime cost depends on hardware, libraries, batch size, and kernel implementations. Values marked "N/A" indicate the session lacked those variables/files at generation time.
"""
story += [Paragraph(notes, styles["BodyText"]), Spacer(1, 12)]

doc.build(story)

print("âœ… PDF generated:", pdf_path)
print("âœ… CSV generated:", csv_path)

"""## TP 06  Task 3.1 â€” Convert and Quantize the MLP Model"""

import tensorflow as tf
import numpy as np
import os

# --- Helper function: representative dataset generator ---
def representative_data_gen():
    # Ù†Ø£Ø®Ø° Ø¹ÙŠÙ†Ø© ØµØºÙŠØ±Ø© Ù…Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (100 Ù…Ø«Ø§Ù„ ÙÙ‚Ø·) Ù„Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ù†Ø·Ø§Ù‚
    for i in range(100):
        img = x_train_mlp[i].astype(np.float32)
        yield [np.expand_dims(img, axis=0)]

# --- Convert the MLP model to TFLite with full integer quantization ---
converter = tf.lite.TFLiteConverter.from_keras_model(mlp_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen

# Ù†Ø·Ù„Ø¨ Ø£Ù† ØªÙƒÙˆÙ† ÙƒÙ„ Ø§Ù„Ù‚ÙŠÙ… (inputs/outputs) ØµØ­ÙŠØ­Ø© Int8 Ø¨Ø§Ù„ÙƒØ§Ù…Ù„
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8

# Ø§Ù„ØªØ­ÙˆÙŠÙ„
tflite_model_mlp = converter.convert()

# Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
with open("mlp_model_quantized.tflite", "wb") as f:
    f.write(tflite_model_mlp)

# --- Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø­Ø¬Ù… Ù‚Ø¨Ù„ ÙˆØ¨Ø¹Ø¯ Ø§Ù„ØªØ­ÙˆÙŠÙ„ ---
mlp_h5_size = os.path.getsize("mlp_model.h5") / (1024 * 1024)
mlp_tflite_size = os.path.getsize("mlp_model_quantized.tflite") / (1024 * 1024)

print(f"ğŸ§  MLP Original (.h5) Size: {mlp_h5_size:.2f} MB")
print(f"ğŸ§  MLP Quantized (.tflite) Size: {mlp_tflite_size:.2f} MB")
print(f"ğŸ”» Size Reduction: {(1 - mlp_tflite_size / mlp_h5_size) * 100:.1f}%")

# --- Helper: representative dataset generator for CNN ---
def representative_data_gen_cnn():
    for i in range(100):
        img = x_train_cnn[i].astype(np.float32)
        yield [np.expand_dims(img, axis=0)]

# --- Convert the CNN model to TFLite with full integer quantization ---
converter = tf.lite.TFLiteConverter.from_keras_model(cnn_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen_cnn

converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8

# Ø§Ù„ØªØ­ÙˆÙŠÙ„
tflite_model_cnn = converter.convert()

# Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
with open("cnn_model_quantized.tflite", "wb") as f:
    f.write(tflite_model_cnn)

# --- Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø­Ø¬Ù… ---
cnn_h5_size = os.path.getsize("cnn_model.h5") / (1024 * 1024)
cnn_tflite_size = os.path.getsize("cnn_model_quantized.tflite") / (1024 * 1024)

print(f"ğŸ§© CNN Original (.h5) Size: {cnn_h5_size:.2f} MB")
print(f"ğŸ§© CNN Quantized (.tflite) Size: {cnn_tflite_size:.2f} MB")
print(f"ğŸ”» Size Reduction: {(1 - cnn_tflite_size / cnn_h5_size) * 100:.1f}%")

"""# **4) Deployment Feasibility Analysis**

1) Memory Constraint (SRAM 512 KB)


MLP (int8 ~0.28 MB / 287 KB):
ÙŠÙ„Ø²Ù… Ø£ÙŠØ¶Ù‹Ø§ Ø¨Ø¶Ø¹ Ø¹Ø´Ø±Ø§Øª Ø¥Ù„Ù‰ Ù…Ø¦Ø§Øª KB Ù„Ù„Ù€ Tensor Arena (ØªÙ†Ø´ÙŠØ·Ø§Øª Ø§Ù„Ø·Ø¨Ù‚Ø§Øª/Ø§Ù„ÙˆØ³Ø§Ø¦Ø·). Ù…Ø¹ Ø¨Ù†ÙŠØ© MLP Ù„Ø¯ÙŠÙ†Ø§ (Flatten â†’ Dense(256) â†’ Dense(128) â†’ Dense(10))ØŒ Ø­Ø¬Ù… Ø§Ù„ØªÙ†Ø´ÙŠØ·Ø§Øª ØµØºÙŠØ± Ù†Ø³Ø¨ÙŠÙ‹Ø§ØŒ Ù„Ø°Ù„Ùƒ ÙŠØ¸Ù„ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø¶Ù…Ù† 512 KB ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª TinyML Ø§Ù„Ù…Ø¹ØªØ§Ø¯Ø©. Ø§Ù„Ù†ØªÙŠØ¬Ø©: Ù…Ù…ÙƒÙ†.


CNN (int8 ~0.07 MB / 72 KB):
Ù„Ù„ØªÙ†Ø´ÙŠØ·Ø§Øª Ø£Ø­Ø¬Ø§Ù… Ù…Ø«Ù„ 26Ã—26Ã—16 Ùˆ 11Ã—11Ã—32ØŒ ÙˆÙ‡ÙŠ ØµØºÙŠØ±Ø© Ù„ØµÙˆØ± 28Ã—28. Ø­ØªÙ‰ Ù…Ø¹ Ù‡ÙˆØ§Ù…Ø´ Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù„Ù€ arenaØŒ ÙŠØ¸Ù„ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø£Ù‚Ù„ Ø¨ÙƒØ«ÙŠØ± Ù…Ù† 512 KB. Ø§Ù„Ù†ØªÙŠØ¬Ø©: Ù…Ù…ÙƒÙ† Ø¨Ø³Ù‡ÙˆÙ„Ø©.


Ø®Ù„Ø§ØµØ© Ø§Ù„Ø°Ø§ÙƒØ±Ø©: ÙƒÙ„Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ÙŠÙ† Ù‚Ø§Ø¨Ù„Ø§Ù† Ù„Ù„ØªØ´ØºÙŠÙ„ Ø¹Ù„Ù‰ XIAO ESP32S3 Ø¨Ø¹Ø¯ Ø§Ù„ÙƒÙ…Ù‘ÙŠØ© Ø§Ù„ÙƒØ§Ù…Ù„Ø©ØŒ ÙˆØ§Ù„Ù€ CNN Ù„Ø¯ÙŠÙ‡ Ù‡Ø§Ù…Ø´ Ø£ÙƒØ¨Ø± Ø¨ÙƒØ«ÙŠØ±.

2) Performance (Latency < ~100 msØŸ)




MLP (inference) â‰ˆ 0.23M FLOPs


CNN (inference) â‰ˆ 1.00M FLOPs




Ù…Ø¹ ESP32-S3 Ø«Ù†Ø§Ø¦ÙŠ Ø§Ù„Ù†ÙˆØ§Ø© @ 240 MHz ÙˆØ¹Ù…Ù„ÙŠØ§Øª int8 (ÙˆÙˆØ¬ÙˆØ¯ ØªØ³Ø±ÙŠØ¹ Ù…ØªØ¬Ù‡ Ø¹Ù„Ù‰ S3)ØŒ ØªØ­Ù‚ÙŠÙ‚ Ù…Ø¹Ø¯Ù„ Ø£Ù‚Ù„ Ù…Ù† 100 ms Ù„Ù…Ø¯Ø®Ù„ 28Ã—28 ÙˆØ§Ù‚Ø¹ÙŠ Ø¬Ø¯Ù‹Ø§:


MLP: Ø¨Ø¶Ø¹ Ù…ÙŠÙ„ÙŠ Ø«ÙˆØ§Ù†Ù Ø¥Ù„Ù‰ Ø¹Ø´Ø±Ø§Øª Ù‚Ù„ÙŠÙ„Ø© Ù…Ù† ms.


CNN: Ø¹Ø´Ø±Ø§Øª Ù‚Ù„ÙŠÙ„Ø© Ù…Ù† ms Ø¹Ø§Ø¯Ø©ØŒ ÙˆØ­ØªÙ‰ ÙÙŠ Ø£Ø³ÙˆØ£ Ø§Ù„Ø£Ø­ÙˆØ§Ù„ ØªØ¨Ù‚Ù‰ Ø¶Ù…Ù† ~100 ms Ù„Ù…Ø¯Ø®Ù„ ÙˆØ§Ø­Ø¯.




Ø®Ù„Ø§ØµØ© Ø§Ù„Ø£Ø¯Ø§Ø¡: Ù†Ø¹Ù…ØŒ Ø§Ù„Ø²Ù…Ù† Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ (â‰¤100 ms Ù„Ù„ØµÙˆØ±Ø©) Ù…ØªÙˆÙ‚Ø¹ Ù„ÙƒÙ„Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ÙŠÙ†ØŒ ÙˆØ§Ù„Ù€ CNN Ø³ÙŠÙ‚Ø¯Ù‘Ù… Ø¯Ù‚Ø© Ø£Ø¹Ù„Ù‰ Ù…Ø¹ Ø²Ù…Ù† Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù…Ù‚Ø¨ÙˆÙ„ Ø¬Ø¯Ù‹Ø§ Ø¹Ù„Ù‰ S3.



Ù‡Ù„ ÙŠÙ…ÙƒÙ† ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ÙŠÙ† Ø¹Ù„Ù‰ XIAO ESP32S3ØŸ
Ù†Ø¹Ù… â€” Ø¨Ø¹Ø¯ Full Integer Quantization (int8)ØŒ ÙƒÙ„Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ÙŠÙ† ÙŠÙ„Ø¨Ù‘ÙŠØ§Ù† Ù‚ÙŠØ¯ Ø§Ù„Ø°Ø§ÙƒØ±Ø© 512 KBØŒ ÙˆØ²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ© (â‰¤100 ms Ù„Ù„ØµÙˆØ±Ø©).


Ø£ÙŠÙ‘Ù‡Ù…Ø§ Ø£ÙØ¶Ù„ Ù„Ù„Ù†Ø´Ø±ØŸ
CNN: Ù„Ø£Ù†Ù‡ Ø£Ø¯Ù‚Ù‘ Ø¨ÙƒØ«ÙŠØ± ÙÙŠ Ù…Ù‡Ø§Ù… Ø§Ù„ØµÙˆØ±ØŒ ÙˆØ­Ø¬Ù…Ù‡ Ø¨Ø¹Ø¯ Ø§Ù„ÙƒÙ…Ù‘ÙŠØ© Ø£ØµØºØ± Ø¨ÙƒØ«ÙŠØ± Ù…Ù† 512 KBØŒ ÙˆÙŠÙ…Ù†Ø­ Ù‡Ø§Ù…Ø´Ù‹Ø§ ÙƒØ¨ÙŠØ±Ù‹Ø§ Ù„Ù„Ù€ arena ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©.
"""